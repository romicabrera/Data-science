{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxA1mBK8PR3bsS/G7PM5hi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romicabrera/Data-science/blob/main/Pel%C3%ADculas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEV_5tLNAHT-",
        "outputId": "6acd474a-9920-4add-88db-1809d6f0ff16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de visualizaciones por película:\n",
            "Acción Extrema: 3\n",
            "Drama Profundo: 2\n",
            "Documental de la Naturaleza: 1\n",
            "La Gran Aventura: 2\n",
            "Romance Inesperado: 2\n",
            "\n",
            "Top 3 de películas por tiempo de visualización:\n",
            "Drama Profundo: 290 minutos\n",
            "Acción Extrema: 275 minutos\n",
            "Romance Inesperado: 250 minutos\n",
            "\n",
            "Películas con rating promedio > 4.5:\n",
            "Drama Profundo: 4.85\n",
            "Romance Inesperado: 4.550000000000001\n",
            "\n",
            "Promedio de minutos vistos por género:\n",
            "Animación: 120.00 min\n",
            "Acción: 90.00 min\n",
            "Drama: 150.00 min\n",
            "Romance: 130.00 min\n",
            "Documental: 95.00 min\n",
            "\n",
            "Top 3 usuarios por minutos vistos:\n",
            "Carla: 250 min\n",
            "Luis: 235 min\n",
            "Pedro: 230 min\n",
            "\n",
            "Género más popular: Acción con 3 visualizaciones\n",
            "\n",
            "Película con mayor rating promedio por género:\n",
            "Acción: Acción Extrema (4.00)\n",
            "Drama: Drama Profundo (4.85)\n",
            "Documental: Documental de la Naturaleza (4.10)\n",
            "Animación: La Gran Aventura (4.35)\n",
            "Romance: Romance Inesperado (4.55)\n",
            "\n",
            "Distribución de ratings promedio de películas:\n",
            "4-5: 5\n",
            "\n",
            "Lazy evaluation en Spark significa que las transformaciones no se ejecutan hasta que se llama a una acción (como collect, count, take). Esto permite a Spark optimizar el plan de ejecución.\n",
            "Para optimizar, podemos usar persist() o cache() en RDDs reutilizados varias veces, por ejemplo:\n",
            "    rdd.persist()\n",
            "Esto evita recalcular el RDD en cada acción y mejora el rendimiento si la memoria lo permite.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Análisis de películas más vistas en Chile con Spark RDDs\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "import sys\n",
        "import os\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "# 1. Carga y preprocesamiento de datos\n",
        "# Check if SparkContext is already initialized\n",
        "if 'sc' not in globals() or  sc is None:\n",
        "  sc = SparkContext(appName='PeliculasMasVistasChile')\n",
        "\n",
        "# Cambia la ruta si es necesario\n",
        "data_path = \"peliculas_mas_vistas.csv\"\n",
        "rdd = sc.textFile(data_path)\n",
        "\n",
        "# Eliminar encabezado y convertir a tuplas\n",
        "header = rdd.first()\n",
        "rdd = rdd.filter(lambda x: x != header)\n",
        "def parse_row(row):\n",
        "    usuario, pelicula, minutos, rating, genero = row.split(\",\")\n",
        "    return (usuario, pelicula, int(minutos), float(rating), genero)\n",
        "rdd = rdd.map(parse_row)\n",
        "\n",
        "# 2. Cantidad de visualizaciones por película\n",
        "vis_por_pelicula = rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b) # a = a + b\n",
        "print(\"Cantidad de visualizaciones por película:\")\n",
        "for pelicula, count in vis_por_pelicula.collect():\n",
        "  print(f\"{pelicula}: {count}\")\n",
        "\n",
        "# 3. Tiempo total de visualizaci+on por película (top 3)\n",
        "tiempo_por_pelicula = rdd.map(lambda x: (x[1], x[2])).reduceByKey(lambda a, b: a + b)\n",
        "top3 = tiempo_por_pelicula.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(\"\\nTop 3 de películas por tiempo de visualización:\")\n",
        "for pelicula, tiempo in top3:\n",
        "  print(f\"{pelicula}: {tiempo} minutos\")\n",
        "\n",
        "# 4. Películas con rating promedio > 4.5\n",
        "rating_sum_count = rdd.map(lambda x: (x[1], (x[3], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "rating_promedio = rating_sum_count.mapValues(lambda x: x[0] / x[1])\n",
        "peliculas_rating_alto = rating_promedio.filter(lambda x: x[1] > 4.5)\n",
        "print(\"\\nPelículas con rating promedio > 4.5:\")\n",
        "for pelicula, rating in peliculas_rating_alto.collect():\n",
        "  print(f\"{pelicula}: {rating}\")\n",
        "\n",
        "# 5. Promedio de minutos vistos por género\n",
        "genero_minutos = rdd.map(lambda x: (x[4], (x[2], 1))).reduceByKey(lambda a, b: a + b)\n",
        "promedio_minutos_genero = genero_minutos.mapValues(lambda x: x[0]/x[1])\n",
        "print(\"\\nPromedio de minutos vistos por género:\")\n",
        "for genero, prom in promedio_minutos_genero.collect():\n",
        "    print(f\"{genero}: {prom:.2f} min\")\n",
        "\n",
        "# 6. Usuarios con mayor tiempo de visualización acumulado (top 3)\n",
        "tiempo_usuario = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)\n",
        "top3_usuarios = tiempo_usuario.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(\"\\nTop 3 usuarios por minutos vistos:\")\n",
        "for usuario, minutos in top3_usuarios:\n",
        "    print(f\"{usuario}: {minutos} min\")\n",
        "\n",
        "# 7. Género más popular (más visualizaciones)\n",
        "genero_vis = rdd.map(lambda x: (x[4], 1)).reduceByKey(lambda a, b: a + b)\n",
        "genero_popular = genero_vis.takeOrdered(1, key=lambda x: -x[1])[0]\n",
        "print(f\"\\nGénero más popular: {genero_popular[0]} con {genero_popular[1]} visualizaciones\")\n",
        "\n",
        "# 8. Película con mayor rating en cada género\n",
        "# (película, (rating, 1, género))\n",
        "genero_pelicula_rating = rdd.map(lambda x: ((x[4], x[1]), (x[3], 1)))\n",
        "sum_count = genero_pelicula_rating.reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
        "promedio = sum_count.mapValues(lambda x: x[0]/x[1])\n",
        "# (género, (película, promedio))\n",
        "genero_peliculas = promedio.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
        "from operator import itemgetter\n",
        "def max_rating(a, b):\n",
        "    return a if a[1] > b[1] else b\n",
        "max_por_genero = genero_peliculas.reduceByKey(max_rating)\n",
        "print(\"\\nPelícula con mayor rating promedio por género:\")\n",
        "for genero, (pelicula, rating) in max_por_genero.collect():\n",
        "    print(f\"{genero}: {pelicula} ({rating:.2f})\")\n",
        "\n",
        "# 9. Distribución de ratings\n",
        "# (película, (rating, 1))\n",
        "peliculas_rating = rdd.map(lambda x: (x[1], (x[3], 1))).reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
        "promedios = peliculas_rating.mapValues(lambda x: x[0]/x[1])\n",
        "def rating_bin(r):\n",
        "    if 1 <= r < 2:\n",
        "        return '1-2'\n",
        "    elif 2 <= r < 3:\n",
        "        return '2-3'\n",
        "    elif 3 <= r < 4:\n",
        "        return '3-4'\n",
        "    elif 4 <= r <= 5:\n",
        "        return '4-5'\n",
        "    else:\n",
        "        return 'otro'\n",
        "dist = promedios.map(lambda x: (rating_bin(x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(\"\\nDistribución de ratings promedio de películas:\")\n",
        "for rango, count in dist.collect():\n",
        "    print(f\"{rango}: {count}\")\n",
        "\n",
        "# 10. Explicación de optimización\n",
        "print(\"\"\"\n",
        "Lazy evaluation en Spark significa que las transformaciones no se ejecutan hasta que se llama a una acción (como collect, count, take). Esto permite a Spark optimizar el plan de ejecución.\n",
        "Para optimizar, podemos usar persist() o cache() en RDDs reutilizados varias veces, por ejemplo:\n",
        "    rdd.persist()\n",
        "Esto evita recalcular el RDD en cada acción y mejora el rendimiento si la memoria lo permite.\n",
        "\"\"\")\n",
        "\n",
        "sc.stop()\n",
        "\n"
      ]
    }
  ]
}